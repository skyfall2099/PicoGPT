{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBMOPrvzgfyV"
      },
      "source": [
        "# PicoGPT: GPT-2 in 60 Lines of NumPy\n",
        "\n",
        "This notebook demonstrates how to implement GPT-2 from scratch using only NumPy.\n",
        "\n",
        "**References:**\n",
        "- Original blog post: [GPT in 60 Lines of NumPy](https://jaykmody.com/blog/gpt-from-scratch/)\n",
        "- Chinese translation: [60行NumPy手搓GPT](https://zhuanlan.zhihu.com/p/640935459)\n",
        "- GitHub: [picoGPT](https://github.com/jaymody/picoGPT)\n",
        "\n",
        "---\n",
        "\n",
        "# PicoGPT：用60行NumPy实现GPT-2\n",
        "\n",
        "本notebook演示如何仅使用NumPy从零实现GPT-2。\n",
        "\n",
        "**参考资料:**\n",
        "- 原文: [GPT in 60 Lines of NumPy](https://jaykmody.com/blog/gpt-from-scratch/)\n",
        "- 中文翻译: [60行NumPy手搓GPT](https://zhuanlan.zhihu.com/p/640935459)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQGWAJwqgfyW"
      },
      "source": [
        "## 1. Install Dependencies / 安装依赖\n",
        "\n",
        "First, let's install the required packages.\n",
        "\n",
        "首先，安装必要的依赖包。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xuODi5UJgfyX",
        "outputId": "60e81e61-1c68-4be5-84fb-f8d8a74d1179"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2026.1.4)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.12.19)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.7.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.76.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.15.1)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.18.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.5)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy regex requests tqdm tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4l8dE9UgfyX"
      },
      "source": [
        "## 2. Import Libraries / 导入库"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bA_kZnjCgfyX"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import re\n",
        "from functools import lru_cache\n",
        "\n",
        "import numpy as np\n",
        "import regex\n",
        "import requests\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rILZ1n4GgfyX"
      },
      "source": [
        "## 3. BPE Tokenizer / BPE分词器\n",
        "\n",
        "GPT-2 uses Byte Pair Encoding (BPE) for tokenization. This code is from OpenAI's official implementation.\n",
        "\n",
        "GPT-2使用字节对编码（BPE）进行分词。这段代码来自OpenAI的官方实现。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "lBK1wCZsgfyX"
      },
      "outputs": [],
      "source": [
        "@lru_cache()\n",
        "def bytes_to_unicode():\n",
        "    \"\"\"\n",
        "    Returns list of utf-8 byte and a corresponding list of unicode strings.\n",
        "    返回utf-8字节和对应的unicode字符串列表。\n",
        "    \"\"\"\n",
        "    bs = list(range(ord(\"!\"), ord(\"~\") + 1)) + list(range(ord(\"¡\"), ord(\"¬\") + 1)) + list(range(ord(\"®\"), ord(\"ÿ\") + 1))\n",
        "    cs = bs[:]\n",
        "    n = 0\n",
        "    for b in range(2**8):\n",
        "        if b not in bs:\n",
        "            bs.append(b)\n",
        "            cs.append(2**8 + n)\n",
        "            n += 1\n",
        "    cs = [chr(n) for n in cs]\n",
        "    return dict(zip(bs, cs))\n",
        "\n",
        "\n",
        "def get_pairs(word):\n",
        "    \"\"\"Return set of symbol pairs in a word. / 返回单词中的符号对集合\"\"\"\n",
        "    pairs = set()\n",
        "    prev_char = word[0]\n",
        "    for char in word[1:]:\n",
        "        pairs.add((prev_char, char))\n",
        "        prev_char = char\n",
        "    return pairs\n",
        "\n",
        "\n",
        "class Encoder:\n",
        "    \"\"\"BPE Encoder for GPT-2 / GPT-2的BPE编码器\"\"\"\n",
        "\n",
        "    def __init__(self, encoder, bpe_merges, errors=\"replace\"):\n",
        "        self.encoder = encoder\n",
        "        self.decoder = {v: k for k, v in self.encoder.items()}\n",
        "        self.errors = errors\n",
        "        self.byte_encoder = bytes_to_unicode()\n",
        "        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}\n",
        "        self.bpe_ranks = dict(zip(bpe_merges, range(len(bpe_merges))))\n",
        "        self.cache = {}\n",
        "        self.pat = regex.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
        "\n",
        "    def bpe(self, token):\n",
        "        if token in self.cache:\n",
        "            return self.cache[token]\n",
        "        word = tuple(token)\n",
        "        pairs = get_pairs(word)\n",
        "\n",
        "        if not pairs:\n",
        "            return token\n",
        "\n",
        "        while True:\n",
        "            bigram = min(pairs, key=lambda pair: self.bpe_ranks.get(pair, float(\"inf\")))\n",
        "            if bigram not in self.bpe_ranks:\n",
        "                break\n",
        "            first, second = bigram\n",
        "            new_word = []\n",
        "            i = 0\n",
        "            while i < len(word):\n",
        "                try:\n",
        "                    j = word.index(first, i)\n",
        "                    new_word.extend(word[i:j])\n",
        "                    i = j\n",
        "                except:\n",
        "                    new_word.extend(word[i:])\n",
        "                    break\n",
        "\n",
        "                if word[i] == first and i < len(word) - 1 and word[i + 1] == second:\n",
        "                    new_word.append(first + second)\n",
        "                    i += 2\n",
        "                else:\n",
        "                    new_word.append(word[i])\n",
        "                    i += 1\n",
        "            new_word = tuple(new_word)\n",
        "            word = new_word\n",
        "            if len(word) == 1:\n",
        "                break\n",
        "            else:\n",
        "                pairs = get_pairs(word)\n",
        "        word = \" \".join(word)\n",
        "        self.cache[token] = word\n",
        "        return word\n",
        "\n",
        "    def encode(self, text):\n",
        "        \"\"\"Encode text to token ids / 将文本编码为token id\"\"\"\n",
        "        bpe_tokens = []\n",
        "        for token in regex.findall(self.pat, text):\n",
        "            token = \"\".join(self.byte_encoder[b] for b in token.encode(\"utf-8\"))\n",
        "            bpe_tokens.extend(self.encoder[bpe_token] for bpe_token in self.bpe(token).split(\" \"))\n",
        "        return bpe_tokens\n",
        "\n",
        "    def decode(self, tokens):\n",
        "        \"\"\"Decode token ids to text / 将token id解码为文本\"\"\"\n",
        "        text = \"\".join([self.decoder[token] for token in tokens])\n",
        "        text = bytearray([self.byte_decoder[c] for c in text]).decode(\"utf-8\", errors=self.errors)\n",
        "        return text\n",
        "\n",
        "\n",
        "def get_encoder(model_name, models_dir):\n",
        "    \"\"\"Load encoder from files / 从文件加载编码器\"\"\"\n",
        "    with open(os.path.join(models_dir, model_name, \"encoder.json\"), \"r\") as f:\n",
        "        encoder = json.load(f)\n",
        "    with open(os.path.join(models_dir, model_name, \"vocab.bpe\"), \"r\", encoding=\"utf-8\") as f:\n",
        "        bpe_data = f.read()\n",
        "    bpe_merges = [tuple(merge_str.split()) for merge_str in bpe_data.split(\"\\n\")[1:-1]]\n",
        "    return Encoder(encoder=encoder, bpe_merges=bpe_merges)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6ttUC6WgfyY"
      },
      "source": [
        "## 4. Model Loading Utilities / 模型加载工具\n",
        "\n",
        "Functions to download and load pre-trained GPT-2 weights from OpenAI.\n",
        "\n",
        "下载和加载OpenAI预训练GPT-2权重的函数。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "gpjK1nYbgfyY"
      },
      "outputs": [],
      "source": [
        "def download_gpt2_files(model_size, model_dir):\n",
        "    \"\"\"Download GPT-2 model files from OpenAI / 从OpenAI下载GPT-2模型文件\"\"\"\n",
        "    assert model_size in [\"124M\", \"355M\", \"774M\", \"1558M\"]\n",
        "    for filename in [\n",
        "        \"checkpoint\",\n",
        "        \"encoder.json\",\n",
        "        \"hparams.json\",\n",
        "        \"model.ckpt.data-00000-of-00001\",\n",
        "        \"model.ckpt.index\",\n",
        "        \"model.ckpt.meta\",\n",
        "        \"vocab.bpe\",\n",
        "    ]:\n",
        "        url = \"https://openaipublic.blob.core.windows.net/gpt-2/models\"\n",
        "        r = requests.get(f\"{url}/{model_size}/{filename}\", stream=True)\n",
        "        r.raise_for_status()\n",
        "\n",
        "        with open(os.path.join(model_dir, filename), \"wb\") as f:\n",
        "            file_size = int(r.headers[\"content-length\"])\n",
        "            chunk_size = 1000\n",
        "            with tqdm(\n",
        "                ncols=100,\n",
        "                desc=\"Fetching \" + filename,\n",
        "                total=file_size,\n",
        "                unit_scale=True,\n",
        "                unit=\"b\",\n",
        "            ) as pbar:\n",
        "                for chunk in r.iter_content(chunk_size=chunk_size):\n",
        "                    f.write(chunk)\n",
        "                    pbar.update(chunk_size)\n",
        "\n",
        "\n",
        "def load_gpt2_params_from_tf_ckpt(tf_ckpt_path, hparams):\n",
        "    \"\"\"Load GPT-2 parameters from TensorFlow checkpoint / 从TensorFlow检查点加载GPT-2参数\"\"\"\n",
        "    def set_in_nested_dict(d, keys, val):\n",
        "        if not keys:\n",
        "            return val\n",
        "        if keys[0] not in d:\n",
        "            d[keys[0]] = {}\n",
        "        d[keys[0]] = set_in_nested_dict(d[keys[0]], keys[1:], val)\n",
        "        return d\n",
        "\n",
        "    params = {\"blocks\": [{} for _ in range(hparams[\"n_layer\"])]}\n",
        "    for name, _ in tf.train.list_variables(tf_ckpt_path):\n",
        "        array = np.squeeze(tf.train.load_variable(tf_ckpt_path, name))\n",
        "        name = name[len(\"model/\") :]\n",
        "        if name.startswith(\"h\"):\n",
        "            m = re.match(r\"h([0-9]+)/(.*)\", name)\n",
        "            n = int(m[1])\n",
        "            sub_name = m[2]\n",
        "            set_in_nested_dict(params[\"blocks\"][n], sub_name.split(\"/\"), array)\n",
        "        else:\n",
        "            set_in_nested_dict(params, name.split(\"/\"), array)\n",
        "\n",
        "    return params\n",
        "\n",
        "\n",
        "def load_encoder_hparams_and_params(model_size, models_dir):\n",
        "    \"\"\"Load encoder, hyperparameters and model parameters / 加载编码器、超参数和模型参数\"\"\"\n",
        "    assert model_size in [\"124M\", \"355M\", \"774M\", \"1558M\"]\n",
        "\n",
        "    model_dir = os.path.join(models_dir, model_size)\n",
        "    tf_ckpt_path = tf.train.latest_checkpoint(model_dir)\n",
        "    if not tf_ckpt_path:  # download files if necessary\n",
        "        os.makedirs(model_dir, exist_ok=True)\n",
        "        download_gpt2_files(model_size, model_dir)\n",
        "        tf_ckpt_path = tf.train.latest_checkpoint(model_dir)\n",
        "\n",
        "    encoder = get_encoder(model_size, models_dir)\n",
        "    hparams = json.load(open(os.path.join(model_dir, \"hparams.json\")))\n",
        "    params = load_gpt2_params_from_tf_ckpt(tf_ckpt_path, hparams)\n",
        "\n",
        "    return encoder, hparams, params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1DYvhpHgfyY"
      },
      "source": [
        "## 5. GPT-2 Model Implementation / GPT-2模型实现\n",
        "\n",
        "Now for the exciting part - the actual GPT-2 implementation in pure NumPy!\n",
        "\n",
        "现在是激动人心的部分——用纯NumPy实现GPT-2！\n",
        "\n",
        "### 5.1 Basic Building Blocks / 基础构建块"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "KPppqsKigfyY"
      },
      "outputs": [],
      "source": [
        "def gelu(x):\n",
        "    \"\"\"\n",
        "    GELU activation function (Gaussian Error Linear Unit)\n",
        "    GELU激活函数（高斯误差线性单元）\n",
        "\n",
        "    This is the activation function used in GPT-2, which is smoother than ReLU.\n",
        "    这是GPT-2使用的激活函数，比ReLU更平滑。\n",
        "    \"\"\"\n",
        "    return 0.5 * x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * x**3)))\n",
        "\n",
        "\n",
        "def softmax(x):\n",
        "    \"\"\"\n",
        "    Softmax function with numerical stability.\n",
        "    带数值稳定性的Softmax函数。\n",
        "\n",
        "    Subtracting max(x) prevents overflow when computing exp(x).\n",
        "    减去max(x)可以防止计算exp(x)时溢出。\n",
        "    \"\"\"\n",
        "    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
        "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
        "\n",
        "\n",
        "def layer_norm(x, g, b, eps=1e-5):\n",
        "    \"\"\"\n",
        "    Layer Normalization\n",
        "    层归一化\n",
        "\n",
        "    Normalizes the input to have mean=0 and variance=1, then scales and shifts.\n",
        "    将输入归一化为均值=0和方差=1，然后进行缩放和偏移。\n",
        "    \"\"\"\n",
        "    mean = np.mean(x, axis=-1, keepdims=True)\n",
        "    variance = np.var(x, axis=-1, keepdims=True)\n",
        "    x = (x - mean) / np.sqrt(variance + eps)  # normalize\n",
        "    return g * x + b  # scale and offset with gamma/beta params\n",
        "\n",
        "\n",
        "def linear(x, w, b):\n",
        "    \"\"\"\n",
        "    Linear transformation: y = xW + b\n",
        "    线性变换: y = xW + b\n",
        "\n",
        "    Shape: [m, in] @ [in, out] + [out] -> [m, out]\n",
        "    \"\"\"\n",
        "    return x @ w + b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ba0kWMSgfyY"
      },
      "source": [
        "### 5.2 Feed-Forward Network / 前馈网络"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "WGEdJH1SgfyY"
      },
      "outputs": [],
      "source": [
        "def ffn(x, c_fc, c_proj):\n",
        "    \"\"\"\n",
        "    Position-wise Feed-Forward Network\n",
        "    位置前馈网络\n",
        "\n",
        "    Shape: [n_seq, n_embd] -> [n_seq, n_embd]\n",
        "\n",
        "    Two linear transformations with GELU activation in between.\n",
        "    两个线性变换，中间是GELU激活。\n",
        "\n",
        "    1. Project up: n_embd -> 4*n_embd\n",
        "    2. GELU activation\n",
        "    3. Project back down: 4*n_embd -> n_embd\n",
        "    \"\"\"\n",
        "    # project up\n",
        "    a = gelu(linear(x, **c_fc))  # [n_seq, n_embd] -> [n_seq, 4*n_embd]\n",
        "\n",
        "    # project back down\n",
        "    x = linear(a, **c_proj)  # [n_seq, 4*n_embd] -> [n_seq, n_embd]\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5jIOnydgfyZ"
      },
      "source": [
        "### 5.3 Attention Mechanism / 注意力机制\n",
        "\n",
        "The core of the Transformer architecture.\n",
        "\n",
        "Transformer架构的核心。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "3HnXKo40gfyZ"
      },
      "outputs": [],
      "source": [
        "def attention(q, k, v, mask):\n",
        "    \"\"\"\n",
        "    Scaled Dot-Product Attention\n",
        "    缩放点积注意力\n",
        "\n",
        "    Shape: [n_q, d_k], [n_k, d_k], [n_k, d_v], [n_q, n_k] -> [n_q, d_v]\n",
        "\n",
        "    Attention(Q,K,V) = softmax(QK^T / sqrt(d_k)) * V\n",
        "    \"\"\"\n",
        "    return softmax(q @ k.T / np.sqrt(q.shape[-1]) + mask) @ v\n",
        "\n",
        "\n",
        "def mha(x, c_attn, c_proj, n_head):\n",
        "    \"\"\"\n",
        "    Multi-Head Attention\n",
        "    多头注意力\n",
        "\n",
        "    Shape: [n_seq, n_embd] -> [n_seq, n_embd]\n",
        "\n",
        "    1. Project input to Q, K, V\n",
        "    2. Split into multiple heads\n",
        "    3. Apply attention on each head with causal mask\n",
        "    4. Concatenate heads\n",
        "    5. Project output\n",
        "    \"\"\"\n",
        "    # qkv projection\n",
        "    x = linear(x, **c_attn)  # [n_seq, n_embd] -> [n_seq, 3*n_embd]\n",
        "\n",
        "    # split into qkv\n",
        "    qkv = np.split(x, 3, axis=-1)  # [n_seq, 3*n_embd] -> [3, n_seq, n_embd]\n",
        "\n",
        "    # split into heads\n",
        "    qkv_heads = list(map(lambda x: np.split(x, n_head, axis=-1), qkv))  # [3, n_head, n_seq, n_embd/n_head]\n",
        "\n",
        "    # causal mask to hide future inputs from being attended to\n",
        "    # 因果掩码，防止模型看到未来的token\n",
        "    causal_mask = (1 - np.tri(x.shape[0], dtype=x.dtype)) * -1e10  # [n_seq, n_seq]\n",
        "\n",
        "    # perform attention over each head\n",
        "    out_heads = [attention(q, k, v, causal_mask) for q, k, v in zip(*qkv_heads)]\n",
        "\n",
        "    # merge heads\n",
        "    x = np.hstack(out_heads)  # [n_head, n_seq, n_embd/n_head] -> [n_seq, n_embd]\n",
        "\n",
        "    # out projection\n",
        "    x = linear(x, **c_proj)  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jz4vBKNgfyZ"
      },
      "source": [
        "### 5.4 Transformer Block / Transformer块"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "iPmlIGBMgfyZ"
      },
      "outputs": [],
      "source": [
        "def transformer_block(x, mlp, attn, ln_1, ln_2, n_head):\n",
        "    \"\"\"\n",
        "    Single Transformer Block\n",
        "    单个Transformer块\n",
        "\n",
        "    Shape: [n_seq, n_embd] -> [n_seq, n_embd]\n",
        "\n",
        "    Each block consists of:\n",
        "    每个块包含：\n",
        "    1. Layer Norm + Multi-Head Attention + Residual Connection\n",
        "       层归一化 + 多头注意力 + 残差连接\n",
        "    2. Layer Norm + Feed-Forward Network + Residual Connection\n",
        "       层归一化 + 前馈网络 + 残差连接\n",
        "    \"\"\"\n",
        "    # multi-head causal self attention\n",
        "    x = x + mha(layer_norm(x, **ln_1), **attn, n_head=n_head)\n",
        "\n",
        "    # position-wise feed forward network\n",
        "    x = x + ffn(layer_norm(x, **ln_2), **mlp)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fi3Jsp56gfyZ"
      },
      "source": [
        "### 5.5 GPT-2 Forward Pass / GPT-2前向传播"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "L04qQhOggfyZ"
      },
      "outputs": [],
      "source": [
        "def gpt2(inputs, wte, wpe, blocks, ln_f, n_head):\n",
        "    \"\"\"\n",
        "    GPT-2 Model Forward Pass\n",
        "    GPT-2模型前向传播\n",
        "\n",
        "    Shape: [n_seq] -> [n_seq, n_vocab]\n",
        "\n",
        "    1. Token embeddings + Positional embeddings\n",
        "       词嵌入 + 位置嵌入\n",
        "    2. Pass through N transformer blocks\n",
        "       通过N个Transformer块\n",
        "    3. Final layer norm\n",
        "       最终层归一化\n",
        "    4. Project to vocabulary\n",
        "       投影到词表\n",
        "    \"\"\"\n",
        "    # token + positional embeddings\n",
        "    x = wte[inputs] + wpe[range(len(inputs))]  # [n_seq] -> [n_seq, n_embd]\n",
        "\n",
        "    # forward pass through n_layer transformer blocks\n",
        "    for block in blocks:\n",
        "        x = transformer_block(x, **block, n_head=n_head)\n",
        "\n",
        "    # projection to vocab (using tied weights with wte)\n",
        "    x = layer_norm(x, **ln_f)  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
        "    return x @ wte.T  # [n_seq, n_embd] -> [n_seq, n_vocab]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEfublU4gfyZ"
      },
      "source": [
        "### 5.6 Text Generation / 文本生成"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "EJAURDHogfyZ"
      },
      "outputs": [],
      "source": [
        "def generate(inputs, params, n_head, n_tokens_to_generate):\n",
        "    \"\"\"\n",
        "    Auto-regressive text generation using greedy sampling.\n",
        "    使用贪婪采样的自回归文本生成。\n",
        "\n",
        "    For each new token:\n",
        "    对于每个新token：\n",
        "    1. Run forward pass to get logits\n",
        "       运行前向传播获取logits\n",
        "    2. Take argmax of last position's logits (greedy sampling)\n",
        "       取最后位置logits的argmax（贪婪采样）\n",
        "    3. Append new token to input\n",
        "       将新token添加到输入\n",
        "    4. Repeat\n",
        "       重复\n",
        "    \"\"\"\n",
        "    for _ in tqdm(range(n_tokens_to_generate), \"generating\"):\n",
        "        logits = gpt2(inputs, **params, n_head=n_head)  # model forward pass\n",
        "        next_id = np.argmax(logits[-1])  # greedy sampling\n",
        "        inputs.append(int(next_id))  # append prediction to input\n",
        "\n",
        "    return inputs[len(inputs) - n_tokens_to_generate :]  # only return generated ids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8l7scGTigfyZ"
      },
      "source": [
        "## 6. Main Function / 主函数\n",
        "\n",
        "Put it all together!\n",
        "\n",
        "整合所有部分！"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "-UjxrpVFgfyZ"
      },
      "outputs": [],
      "source": [
        "def main(prompt, n_tokens_to_generate=40, model_size=\"124M\", models_dir=\"models\"):\n",
        "    \"\"\"\n",
        "    Main function to generate text.\n",
        "    生成文本的主函数。\n",
        "\n",
        "    Args:\n",
        "        prompt: Input text prompt / 输入文本提示\n",
        "        n_tokens_to_generate: Number of tokens to generate / 要生成的token数量\n",
        "        model_size: Model size (124M, 355M, 774M, 1558M) / 模型大小\n",
        "        models_dir: Directory to store/load models / 存储/加载模型的目录\n",
        "    \"\"\"\n",
        "    # load encoder, hparams, and params from the released open-ai gpt-2 files\n",
        "    # 从OpenAI发布的GPT-2文件加载编码器、超参数和参数\n",
        "    encoder, hparams, params = load_encoder_hparams_and_params(model_size, models_dir)\n",
        "\n",
        "    # encode the input string using the BPE tokenizer\n",
        "    # 使用BPE分词器编码输入字符串\n",
        "    input_ids = encoder.encode(prompt)\n",
        "\n",
        "    # make sure we are not surpassing the max sequence length of our model\n",
        "    # 确保我们没有超过模型的最大序列长度\n",
        "    assert len(input_ids) + n_tokens_to_generate < hparams[\"n_ctx\"]\n",
        "\n",
        "    # generate output ids\n",
        "    # 生成输出id\n",
        "    output_ids = generate(input_ids, params, hparams[\"n_head\"], n_tokens_to_generate)\n",
        "\n",
        "    # decode the ids back into a string\n",
        "    # 将id解码回字符串\n",
        "    output_text = encoder.decode(output_ids)\n",
        "\n",
        "    return output_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r19T3N20gfyZ"
      },
      "source": [
        "## 7. Run Demo / 运行演示\n",
        "\n",
        "Let's test our GPT-2 implementation! The first run will download the model (~500MB for 124M).\n",
        "\n",
        "让我们测试GPT-2实现！首次运行会下载模型（124M约500MB）。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qGlsuYs8gfyZ",
        "outputId": "5ea467aa-3527-4fe6-d05e-3800c24c2d54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: Alan Turing theorized that computers would one day become\n",
            "\n",
            "Generating...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching checkpoint: 1.00kb [00:00, 4.54Mb/s]                                                       \n",
            "Fetching encoder.json: 1.04Mb [00:01, 741kb/s]                                                      \n",
            "Fetching hparams.json: 1.00kb [00:00, 6.04Mb/s]                                                     \n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mb [03:29, 2.37Mb/s]                                    \n",
            "Fetching model.ckpt.index: 6.00kb [00:00, 19.4Mb/s]                                                 \n",
            "Fetching model.ckpt.meta: 472kb [00:01, 326kb/s]                                                    \n",
            "Fetching vocab.bpe: 457kb [00:00, 458kb/s]                                                          \n",
            "generating: 100%|██████████| 40/40 [00:46<00:00,  1.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Generated text:  the most powerful machines on the planet.\n",
            "\n",
            "The computer is a machine that can perform complex calculations, and it can perform these calculations in a way that is very similar to the human brain.\n",
            "\n",
            "==================================================\n",
            "\n",
            "Full output: Alan Turing theorized that computers would one day become the most powerful machines on the planet.\n",
            "\n",
            "The computer is a machine that can perform complex calculations, and it can perform these calculations in a way that is very similar to the human brain.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Example 1: Alan Turing quote\n",
        "# 示例1：艾伦·图灵名言\n",
        "prompt = \"Alan Turing theorized that computers would one day become\"\n",
        "print(f\"Prompt: {prompt}\")\n",
        "print(f\"\\nGenerating...\\n\")\n",
        "\n",
        "output = main(prompt, n_tokens_to_generate=40)\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(f\"Generated text: {output}\")\n",
        "print(f\"{'='*50}\")\n",
        "print(f\"\\nFull output: {prompt}{output}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XLR9umZCgfyZ",
        "outputId": "abeefd98-34ff-41f1-8127-7afe32af5d1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: The meaning of life is\n",
            "\n",
            "Generating...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "generating: 100%|██████████| 50/50 [00:58<00:00,  1.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Full output: The meaning of life is not the same as the meaning of death.\n",
            "\n",
            "The meaning of life is not the same as the meaning of death.\n",
            "\n",
            "The meaning of life is not the same as the meaning of death.\n",
            "\n",
            "The meaning of life is not the\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Example 2: Try your own prompt!\n",
        "# 示例2：尝试你自己的提示！\n",
        "\n",
        "# Change this to whatever you want\n",
        "# 改成你想要的任何内容\n",
        "your_prompt = \"The meaning of life is\"\n",
        "\n",
        "print(f\"Prompt: {your_prompt}\")\n",
        "print(f\"\\nGenerating...\\n\")\n",
        "\n",
        "output = main(your_prompt, n_tokens_to_generate=50)\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(f\"Full output: {your_prompt}{output}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqquRUyCgfyZ"
      },
      "source": [
        "## 8. Model Architecture Summary / 模型架构总结\n",
        "\n",
        "```\n",
        "GPT-2 Architecture (124M model):\n",
        "GPT-2架构（124M模型）:\n",
        "\n",
        "├── Token Embedding (wte): [50257, 768]     # 词嵌入\n",
        "├── Position Embedding (wpe): [1024, 768]   # 位置嵌入\n",
        "├── 12 Transformer Blocks                   # 12个Transformer块\n",
        "│   ├── Layer Norm 1                        # 层归一化1\n",
        "│   ├── Multi-Head Attention (12 heads)     # 多头注意力（12头）\n",
        "│   │   ├── Q, K, V Projection              # Q、K、V投影\n",
        "│   │   ├── Scaled Dot-Product Attention    # 缩放点积注意力\n",
        "│   │   └── Output Projection               # 输出投影\n",
        "│   ├── Residual Connection                 # 残差连接\n",
        "│   ├── Layer Norm 2                        # 层归一化2\n",
        "│   ├── Feed-Forward Network                # 前馈网络\n",
        "│   │   ├── Linear (768 -> 3072)            # 线性变换\n",
        "│   │   ├── GELU                            # GELU激活\n",
        "│   │   └── Linear (3072 -> 768)            # 线性变换\n",
        "│   └── Residual Connection                 # 残差连接\n",
        "├── Final Layer Norm                        # 最终层归一化\n",
        "└── Output (tied with wte.T)                # 输出（与wte.T共享权重）\n",
        "\n",
        "Total Parameters / 总参数量:\n",
        "- 124M: 12 layers, 768 hidden, 12 heads\n",
        "- 355M: 24 layers, 1024 hidden, 16 heads\n",
        "- 774M: 36 layers, 1280 hidden, 20 heads\n",
        "- 1558M: 48 layers, 1600 hidden, 25 heads\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFA7W1-HgfyZ"
      },
      "source": [
        "## 9. Key Takeaways / 关键要点\n",
        "\n",
        "1. **The entire GPT-2 forward pass is just ~40 lines of NumPy!**\n",
        "   \n",
        "   整个GPT-2前向传播只需约40行NumPy代码！\n",
        "\n",
        "2. **Core components / 核心组件:**\n",
        "   - `gelu`: Activation function / 激活函数\n",
        "   - `softmax`: Probability distribution / 概率分布\n",
        "   - `layer_norm`: Normalization / 归一化\n",
        "   - `linear`: Matrix multiplication / 矩阵乘法\n",
        "   - `attention`: The key innovation / 关键创新\n",
        "   - `mha`: Multi-head version / 多头版本\n",
        "   - `ffn`: Position-wise processing / 逐位置处理\n",
        "   - `transformer_block`: Combines attention + FFN / 组合注意力+FFN\n",
        "   - `gpt2`: Stack of blocks / 块的堆叠\n",
        "\n",
        "3. **Limitations of this implementation / 此实现的局限性:**\n",
        "   - Inference only (no training) / 仅推理（无训练）\n",
        "   - No batching / 无批处理\n",
        "   - Greedy sampling only / 仅贪婪采样\n",
        "   - Slow (pure NumPy) / 速度慢（纯NumPy）\n",
        "\n",
        "4. **The magic is in the pre-trained weights!**\n",
        "   \n",
        "   魔法在于预训练的权重！"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}