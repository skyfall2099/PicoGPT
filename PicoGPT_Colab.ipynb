{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PicoGPT: GPT-2 in 60 Lines of NumPy\n",
    "\n",
    "This notebook demonstrates how to implement GPT-2 from scratch using only NumPy.\n",
    "\n",
    "**References:**\n",
    "- Original blog post: [GPT in 60 Lines of NumPy](https://jaykmody.com/blog/gpt-from-scratch/)\n",
    "- Chinese translation: [60行NumPy手搓GPT](https://zhuanlan.zhihu.com/p/640935459)\n",
    "- GitHub: [picoGPT](https://github.com/jaymody/picoGPT)\n",
    "\n",
    "---\n",
    "\n",
    "# PicoGPT：用60行NumPy实现GPT-2\n",
    "\n",
    "本notebook演示如何仅使用NumPy从零实现GPT-2。\n",
    "\n",
    "**参考资料:**\n",
    "- 原文: [GPT in 60 Lines of NumPy](https://jaykmody.com/blog/gpt-from-scratch/)\n",
    "- 中文翻译: [60行NumPy手搓GPT](https://zhuanlan.zhihu.com/p/640935459)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies / 安装依赖\n",
    "\n",
    "First, let's install the required packages.\n",
    "\n",
    "首先，安装必要的依赖包。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy regex requests tqdm tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries / 导入库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "from functools import lru_cache\n",
    "\n",
    "import numpy as np\n",
    "import regex\n",
    "import requests\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. BPE Tokenizer / BPE分词器\n",
    "\n",
    "GPT-2 uses Byte Pair Encoding (BPE) for tokenization. This code is from OpenAI's official implementation.\n",
    "\n",
    "GPT-2使用字节对编码（BPE）进行分词。这段代码来自OpenAI的官方实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache()\n",
    "def bytes_to_unicode():\n",
    "    \"\"\"\n",
    "    Returns list of utf-8 byte and a corresponding list of unicode strings.\n",
    "    返回utf-8字节和对应的unicode字符串列表。\n",
    "    \"\"\"\n",
    "    bs = list(range(ord(\"!\"), ord(\"~\") + 1)) + list(range(ord(\"¡\"), ord(\"¬\") + 1)) + list(range(ord(\"®\"), ord(\"ÿ\") + 1))\n",
    "    cs = bs[:]\n",
    "    n = 0\n",
    "    for b in range(2**8):\n",
    "        if b not in bs:\n",
    "            bs.append(b)\n",
    "            cs.append(2**8 + n)\n",
    "            n += 1\n",
    "    cs = [chr(n) for n in cs]\n",
    "    return dict(zip(bs, cs))\n",
    "\n",
    "\n",
    "def get_pairs(word):\n",
    "    \"\"\"Return set of symbol pairs in a word. / 返回单词中的符号对集合\"\"\"\n",
    "    pairs = set()\n",
    "    prev_char = word[0]\n",
    "    for char in word[1:]:\n",
    "        pairs.add((prev_char, char))\n",
    "        prev_char = char\n",
    "    return pairs\n",
    "\n",
    "\n",
    "class Encoder:\n",
    "    \"\"\"BPE Encoder for GPT-2 / GPT-2的BPE编码器\"\"\"\n",
    "    \n",
    "    def __init__(self, encoder, bpe_merges, errors=\"replace\"):\n",
    "        self.encoder = encoder\n",
    "        self.decoder = {v: k for k, v in self.encoder.items()}\n",
    "        self.errors = errors\n",
    "        self.byte_encoder = bytes_to_unicode()\n",
    "        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}\n",
    "        self.bpe_ranks = dict(zip(bpe_merges, range(len(bpe_merges))))\n",
    "        self.cache = {}\n",
    "        self.pat = regex.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
    "\n",
    "    def bpe(self, token):\n",
    "        if token in self.cache:\n",
    "            return self.cache[token]\n",
    "        word = tuple(token)\n",
    "        pairs = get_pairs(word)\n",
    "\n",
    "        if not pairs:\n",
    "            return token\n",
    "\n",
    "        while True:\n",
    "            bigram = min(pairs, key=lambda pair: self.bpe_ranks.get(pair, float(\"inf\")))\n",
    "            if bigram not in self.bpe_ranks:\n",
    "                break\n",
    "            first, second = bigram\n",
    "            new_word = []\n",
    "            i = 0\n",
    "            while i < len(word):\n",
    "                try:\n",
    "                    j = word.index(first, i)\n",
    "                    new_word.extend(word[i:j])\n",
    "                    i = j\n",
    "                except:\n",
    "                    new_word.extend(word[i:])\n",
    "                    break\n",
    "\n",
    "                if word[i] == first and i < len(word) - 1 and word[i + 1] == second:\n",
    "                    new_word.append(first + second)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_word.append(word[i])\n",
    "                    i += 1\n",
    "            new_word = tuple(new_word)\n",
    "            word = new_word\n",
    "            if len(word) == 1:\n",
    "                break\n",
    "            else:\n",
    "                pairs = get_pairs(word)\n",
    "        word = \" \".join(word)\n",
    "        self.cache[token] = word\n",
    "        return word\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"Encode text to token ids / 将文本编码为token id\"\"\"\n",
    "        bpe_tokens = []\n",
    "        for token in regex.findall(self.pat, text):\n",
    "            token = \"\".join(self.byte_encoder[b] for b in token.encode(\"utf-8\"))\n",
    "            bpe_tokens.extend(self.encoder[bpe_token] for bpe_token in self.bpe(token).split(\" \"))\n",
    "        return bpe_tokens\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        \"\"\"Decode token ids to text / 将token id解码为文本\"\"\"\n",
    "        text = \"\".join([self.decoder[token] for token in tokens])\n",
    "        text = bytearray([self.byte_decoder[c] for c in text]).decode(\"utf-8\", errors=self.errors)\n",
    "        return text\n",
    "\n",
    "\n",
    "def get_encoder(model_name, models_dir):\n",
    "    \"\"\"Load encoder from files / 从文件加载编码器\"\"\"\n",
    "    with open(os.path.join(models_dir, model_name, \"encoder.json\"), \"r\") as f:\n",
    "        encoder = json.load(f)\n",
    "    with open(os.path.join(models_dir, model_name, \"vocab.bpe\"), \"r\", encoding=\"utf-8\") as f:\n",
    "        bpe_data = f.read()\n",
    "    bpe_merges = [tuple(merge_str.split()) for merge_str in bpe_data.split(\"\\n\")[1:-1]]\n",
    "    return Encoder(encoder=encoder, bpe_merges=bpe_merges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Loading Utilities / 模型加载工具\n",
    "\n",
    "Functions to download and load pre-trained GPT-2 weights from OpenAI.\n",
    "\n",
    "下载和加载OpenAI预训练GPT-2权重的函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_gpt2_files(model_size, model_dir):\n",
    "    \"\"\"Download GPT-2 model files from OpenAI / 从OpenAI下载GPT-2模型文件\"\"\"\n",
    "    assert model_size in [\"124M\", \"355M\", \"774M\", \"1558M\"]\n",
    "    for filename in [\n",
    "        \"checkpoint\",\n",
    "        \"encoder.json\",\n",
    "        \"hparams.json\",\n",
    "        \"model.ckpt.data-00000-of-00001\",\n",
    "        \"model.ckpt.index\",\n",
    "        \"model.ckpt.meta\",\n",
    "        \"vocab.bpe\",\n",
    "    ]:\n",
    "        url = \"https://openaipublic.blob.core.windows.net/gpt-2/models\"\n",
    "        r = requests.get(f\"{url}/{model_size}/{filename}\", stream=True)\n",
    "        r.raise_for_status()\n",
    "\n",
    "        with open(os.path.join(model_dir, filename), \"wb\") as f:\n",
    "            file_size = int(r.headers[\"content-length\"])\n",
    "            chunk_size = 1000\n",
    "            with tqdm(\n",
    "                ncols=100,\n",
    "                desc=\"Fetching \" + filename,\n",
    "                total=file_size,\n",
    "                unit_scale=True,\n",
    "                unit=\"b\",\n",
    "            ) as pbar:\n",
    "                for chunk in r.iter_content(chunk_size=chunk_size):\n",
    "                    f.write(chunk)\n",
    "                    pbar.update(chunk_size)\n",
    "\n",
    "\n",
    "def load_gpt2_params_from_tf_ckpt(tf_ckpt_path, hparams):\n",
    "    \"\"\"Load GPT-2 parameters from TensorFlow checkpoint / 从TensorFlow检查点加载GPT-2参数\"\"\"\n",
    "    def set_in_nested_dict(d, keys, val):\n",
    "        if not keys:\n",
    "            return val\n",
    "        if keys[0] not in d:\n",
    "            d[keys[0]] = {}\n",
    "        d[keys[0]] = set_in_nested_dict(d[keys[0]], keys[1:], val)\n",
    "        return d\n",
    "\n",
    "    params = {\"blocks\": [{} for _ in range(hparams[\"n_layer\"])]}\n",
    "    for name, _ in tf.train.list_variables(tf_ckpt_path):\n",
    "        array = np.squeeze(tf.train.load_variable(tf_ckpt_path, name))\n",
    "        name = name[len(\"model/\") :]\n",
    "        if name.startswith(\"h\"):\n",
    "            m = re.match(r\"h([0-9]+)/(.*)\", name)\n",
    "            n = int(m[1])\n",
    "            sub_name = m[2]\n",
    "            set_in_nested_dict(params[\"blocks\"][n], sub_name.split(\"/\"), array)\n",
    "        else:\n",
    "            set_in_nested_dict(params, name.split(\"/\"), array)\n",
    "\n",
    "    return params\n",
    "\n",
    "\n",
    "def load_encoder_hparams_and_params(model_size, models_dir):\n",
    "    \"\"\"Load encoder, hyperparameters and model parameters / 加载编码器、超参数和模型参数\"\"\"\n",
    "    assert model_size in [\"124M\", \"355M\", \"774M\", \"1558M\"]\n",
    "\n",
    "    model_dir = os.path.join(models_dir, model_size)\n",
    "    tf_ckpt_path = tf.train.latest_checkpoint(model_dir)\n",
    "    if not tf_ckpt_path:  # download files if necessary\n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "        download_gpt2_files(model_size, model_dir)\n",
    "        tf_ckpt_path = tf.train.latest_checkpoint(model_dir)\n",
    "\n",
    "    encoder = get_encoder(model_size, models_dir)\n",
    "    hparams = json.load(open(os.path.join(model_dir, \"hparams.json\")))\n",
    "    params = load_gpt2_params_from_tf_ckpt(tf_ckpt_path, hparams)\n",
    "\n",
    "    return encoder, hparams, params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. GPT-2 Model Implementation / GPT-2模型实现\n",
    "\n",
    "Now for the exciting part - the actual GPT-2 implementation in pure NumPy!\n",
    "\n",
    "现在是激动人心的部分——用纯NumPy实现GPT-2！\n",
    "\n",
    "### 5.1 Basic Building Blocks / 基础构建块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gelu(x):\n",
    "    \"\"\"\n",
    "    GELU activation function (Gaussian Error Linear Unit)\n",
    "    GELU激活函数（高斯误差线性单元）\n",
    "    \n",
    "    This is the activation function used in GPT-2, which is smoother than ReLU.\n",
    "    这是GPT-2使用的激活函数，比ReLU更平滑。\n",
    "    \"\"\"\n",
    "    return 0.5 * x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * x**3)))\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Softmax function with numerical stability.\n",
    "    带数值稳定性的Softmax函数。\n",
    "    \n",
    "    Subtracting max(x) prevents overflow when computing exp(x).\n",
    "    减去max(x)可以防止计算exp(x)时溢出。\n",
    "    \"\"\"\n",
    "    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "\n",
    "\n",
    "def layer_norm(x, g, b, eps=1e-5):\n",
    "    \"\"\"\n",
    "    Layer Normalization\n",
    "    层归一化\n",
    "    \n",
    "    Normalizes the input to have mean=0 and variance=1, then scales and shifts.\n",
    "    将输入归一化为均值=0和方差=1，然后进行缩放和偏移。\n",
    "    \"\"\"\n",
    "    mean = np.mean(x, axis=-1, keepdims=True)\n",
    "    variance = np.var(x, axis=-1, keepdims=True)\n",
    "    x = (x - mean) / np.sqrt(variance + eps)  # normalize\n",
    "    return g * x + b  # scale and offset with gamma/beta params\n",
    "\n",
    "\n",
    "def linear(x, w, b):\n",
    "    \"\"\"\n",
    "    Linear transformation: y = xW + b\n",
    "    线性变换: y = xW + b\n",
    "    \n",
    "    Shape: [m, in] @ [in, out] + [out] -> [m, out]\n",
    "    \"\"\"\n",
    "    return x @ w + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Feed-Forward Network / 前馈网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ffn(x, c_fc, c_proj):\n",
    "    \"\"\"\n",
    "    Position-wise Feed-Forward Network\n",
    "    位置前馈网络\n",
    "    \n",
    "    Shape: [n_seq, n_embd] -> [n_seq, n_embd]\n",
    "    \n",
    "    Two linear transformations with GELU activation in between.\n",
    "    两个线性变换，中间是GELU激活。\n",
    "    \n",
    "    1. Project up: n_embd -> 4*n_embd\n",
    "    2. GELU activation\n",
    "    3. Project back down: 4*n_embd -> n_embd\n",
    "    \"\"\"\n",
    "    # project up\n",
    "    a = gelu(linear(x, **c_fc))  # [n_seq, n_embd] -> [n_seq, 4*n_embd]\n",
    "\n",
    "    # project back down\n",
    "    x = linear(a, **c_proj)  # [n_seq, 4*n_embd] -> [n_seq, n_embd]\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Attention Mechanism / 注意力机制\n",
    "\n",
    "The core of the Transformer architecture.\n",
    "\n",
    "Transformer架构的核心。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(q, k, v, mask):\n",
    "    \"\"\"\n",
    "    Scaled Dot-Product Attention\n",
    "    缩放点积注意力\n",
    "    \n",
    "    Shape: [n_q, d_k], [n_k, d_k], [n_k, d_v], [n_q, n_k] -> [n_q, d_v]\n",
    "    \n",
    "    Attention(Q,K,V) = softmax(QK^T / sqrt(d_k)) * V\n",
    "    \"\"\"\n",
    "    return softmax(q @ k.T / np.sqrt(q.shape[-1]) + mask) @ v\n",
    "\n",
    "\n",
    "def mha(x, c_attn, c_proj, n_head):\n",
    "    \"\"\"\n",
    "    Multi-Head Attention\n",
    "    多头注意力\n",
    "    \n",
    "    Shape: [n_seq, n_embd] -> [n_seq, n_embd]\n",
    "    \n",
    "    1. Project input to Q, K, V\n",
    "    2. Split into multiple heads\n",
    "    3. Apply attention on each head with causal mask\n",
    "    4. Concatenate heads\n",
    "    5. Project output\n",
    "    \"\"\"\n",
    "    # qkv projection\n",
    "    x = linear(x, **c_attn)  # [n_seq, n_embd] -> [n_seq, 3*n_embd]\n",
    "\n",
    "    # split into qkv\n",
    "    qkv = np.split(x, 3, axis=-1)  # [n_seq, 3*n_embd] -> [3, n_seq, n_embd]\n",
    "\n",
    "    # split into heads\n",
    "    qkv_heads = list(map(lambda x: np.split(x, n_head, axis=-1), qkv))  # [3, n_head, n_seq, n_embd/n_head]\n",
    "\n",
    "    # causal mask to hide future inputs from being attended to\n",
    "    # 因果掩码，防止模型看到未来的token\n",
    "    causal_mask = (1 - np.tri(x.shape[0], dtype=x.dtype)) * -1e10  # [n_seq, n_seq]\n",
    "\n",
    "    # perform attention over each head\n",
    "    out_heads = [attention(q, k, v, causal_mask) for q, k, v in zip(*qkv_heads)]\n",
    "\n",
    "    # merge heads\n",
    "    x = np.hstack(out_heads)  # [n_head, n_seq, n_embd/n_head] -> [n_seq, n_embd]\n",
    "\n",
    "    # out projection\n",
    "    x = linear(x, **c_proj)  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Transformer Block / Transformer块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_block(x, mlp, attn, ln_1, ln_2, n_head):\n",
    "    \"\"\"\n",
    "    Single Transformer Block\n",
    "    单个Transformer块\n",
    "    \n",
    "    Shape: [n_seq, n_embd] -> [n_seq, n_embd]\n",
    "    \n",
    "    Each block consists of:\n",
    "    每个块包含：\n",
    "    1. Layer Norm + Multi-Head Attention + Residual Connection\n",
    "       层归一化 + 多头注意力 + 残差连接\n",
    "    2. Layer Norm + Feed-Forward Network + Residual Connection\n",
    "       层归一化 + 前馈网络 + 残差连接\n",
    "    \"\"\"\n",
    "    # multi-head causal self attention\n",
    "    x = x + mha(layer_norm(x, **ln_1), **attn, n_head=n_head)\n",
    "\n",
    "    # position-wise feed forward network\n",
    "    x = x + ffn(layer_norm(x, **ln_2), **mlp)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 GPT-2 Forward Pass / GPT-2前向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt2(inputs, wte, wpe, blocks, ln_f, n_head):\n",
    "    \"\"\"\n",
    "    GPT-2 Model Forward Pass\n",
    "    GPT-2模型前向传播\n",
    "    \n",
    "    Shape: [n_seq] -> [n_seq, n_vocab]\n",
    "    \n",
    "    1. Token embeddings + Positional embeddings\n",
    "       词嵌入 + 位置嵌入\n",
    "    2. Pass through N transformer blocks\n",
    "       通过N个Transformer块\n",
    "    3. Final layer norm\n",
    "       最终层归一化\n",
    "    4. Project to vocabulary\n",
    "       投影到词表\n",
    "    \"\"\"\n",
    "    # token + positional embeddings\n",
    "    x = wte[inputs] + wpe[range(len(inputs))]  # [n_seq] -> [n_seq, n_embd]\n",
    "\n",
    "    # forward pass through n_layer transformer blocks\n",
    "    for block in blocks:\n",
    "        x = transformer_block(x, **block, n_head=n_head)\n",
    "\n",
    "    # projection to vocab (using tied weights with wte)\n",
    "    x = layer_norm(x, **ln_f)  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
    "    return x @ wte.T  # [n_seq, n_embd] -> [n_seq, n_vocab]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 Text Generation / 文本生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(inputs, params, n_head, n_tokens_to_generate):\n",
    "    \"\"\"\n",
    "    Auto-regressive text generation using greedy sampling.\n",
    "    使用贪婪采样的自回归文本生成。\n",
    "    \n",
    "    For each new token:\n",
    "    对于每个新token：\n",
    "    1. Run forward pass to get logits\n",
    "       运行前向传播获取logits\n",
    "    2. Take argmax of last position's logits (greedy sampling)\n",
    "       取最后位置logits的argmax（贪婪采样）\n",
    "    3. Append new token to input\n",
    "       将新token添加到输入\n",
    "    4. Repeat\n",
    "       重复\n",
    "    \"\"\"\n",
    "    for _ in tqdm(range(n_tokens_to_generate), \"generating\"):\n",
    "        logits = gpt2(inputs, **params, n_head=n_head)  # model forward pass\n",
    "        next_id = np.argmax(logits[-1])  # greedy sampling\n",
    "        inputs.append(int(next_id))  # append prediction to input\n",
    "\n",
    "    return inputs[len(inputs) - n_tokens_to_generate :]  # only return generated ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Main Function / 主函数\n",
    "\n",
    "Put it all together!\n",
    "\n",
    "整合所有部分！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(prompt, n_tokens_to_generate=40, model_size=\"124M\", models_dir=\"models\"):\n",
    "    \"\"\"\n",
    "    Main function to generate text.\n",
    "    生成文本的主函数。\n",
    "    \n",
    "    Args:\n",
    "        prompt: Input text prompt / 输入文本提示\n",
    "        n_tokens_to_generate: Number of tokens to generate / 要生成的token数量\n",
    "        model_size: Model size (124M, 355M, 774M, 1558M) / 模型大小\n",
    "        models_dir: Directory to store/load models / 存储/加载模型的目录\n",
    "    \"\"\"\n",
    "    # load encoder, hparams, and params from the released open-ai gpt-2 files\n",
    "    # 从OpenAI发布的GPT-2文件加载编码器、超参数和参数\n",
    "    encoder, hparams, params = load_encoder_hparams_and_params(model_size, models_dir)\n",
    "\n",
    "    # encode the input string using the BPE tokenizer\n",
    "    # 使用BPE分词器编码输入字符串\n",
    "    input_ids = encoder.encode(prompt)\n",
    "\n",
    "    # make sure we are not surpassing the max sequence length of our model\n",
    "    # 确保我们没有超过模型的最大序列长度\n",
    "    assert len(input_ids) + n_tokens_to_generate < hparams[\"n_ctx\"]\n",
    "\n",
    "    # generate output ids\n",
    "    # 生成输出id\n",
    "    output_ids = generate(input_ids, params, hparams[\"n_head\"], n_tokens_to_generate)\n",
    "\n",
    "    # decode the ids back into a string\n",
    "    # 将id解码回字符串\n",
    "    output_text = encoder.decode(output_ids)\n",
    "\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. Run Demo / 运行演示\n\nLet's test our GPT-2 implementation! The first run will automatically download the pre-trained model weights (~500MB for 124M).\n\n让我们测试GPT-2实现！首次运行会自动下载预训练模型权重（124M约500MB）。\n\n> **About the weights / 关于模型权重:**\n>\n> This notebook implements the GPT-2 **inference (forward pass)** from scratch using NumPy — that's the ~60 lines of code above. However, the model also needs **pre-trained weights** (the billions of numbers learned during training) to produce meaningful output.\n>\n> 本notebook用NumPy从零实现了GPT-2的**推理（前向传播）**——就是上面约60行代码。但模型还需要**预训练权重**（训练过程中学到的数十亿个数字）才能产生有意义的输出。\n>\n> The weights are **openly released by OpenAI** under the MIT license. They are downloaded from OpenAI's public storage:\n> `https://openaipublic.blob.core.windows.net/gpt-2/models/`\n>\n> 这些权重由**OpenAI以MIT许可证开源发布**，从OpenAI的公开存储下载：\n> `https://openaipublic.blob.core.windows.net/gpt-2/models/`\n>\n> We use the **124M** (smallest) version here. Available sizes:\n>\n> 这里使用的是**124M**（最小）版本。可用的模型大小：\n>\n> | Model | Layers | Hidden Size | Heads | Params | Download Size |\n> |-------|--------|-------------|-------|--------|---------------|\n> | 124M  | 12     | 768         | 12    | ~124M  | ~500MB        |\n> | 355M  | 24     | 1024        | 16    | ~355M  | ~1.5GB        |\n> | 774M  | 36     | 1280        | 20    | ~774M  | ~3GB          |\n> | 1558M | 48     | 1600        | 25    | ~1.5B  | ~6GB          |\n>\n> **In short: the code is ours, the weights are OpenAI's.**\n>\n> **简而言之：代码是我们从零实现的，权重是OpenAI开源的。**"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Alan Turing quote\n",
    "# 示例1：艾伦·图灵名言\n",
    "prompt = \"Alan Turing theorized that computers would one day become\"\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"\\nGenerating...\\n\")\n",
    "\n",
    "output = main(prompt, n_tokens_to_generate=40)\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Generated text: {output}\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"\\nFull output: {prompt}{output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Try your own prompt!\n",
    "# 示例2：尝试你自己的提示！\n",
    "\n",
    "# Change this to whatever you want\n",
    "# 改成你想要的任何内容\n",
    "your_prompt = \"The meaning of life is\"\n",
    "\n",
    "print(f\"Prompt: {your_prompt}\")\n",
    "print(f\"\\nGenerating...\\n\")\n",
    "\n",
    "output = main(your_prompt, n_tokens_to_generate=50)\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Full output: {your_prompt}{output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 9. Model Architecture Summary / 模型架构总结\n\n```\nGPT-2 Architecture (124M model):\nGPT-2架构（124M模型）:\n\n├── Token Embedding (wte): [50257, 768]     # 词嵌入\n├── Position Embedding (wpe): [1024, 768]   # 位置嵌入\n├── 12 Transformer Blocks                   # 12个Transformer块\n│   ├── Layer Norm 1                        # 层归一化1\n│   ├── Multi-Head Attention (12 heads)     # 多头注意力（12头）\n│   │   ├── Q, K, V Projection              # Q、K、V投影\n│   │   ├── Scaled Dot-Product Attention    # 缩放点积注意力\n│   │   └── Output Projection               # 输出投影\n│   ├── Residual Connection                 # 残差连接\n│   ├── Layer Norm 2                        # 层归一化2\n│   ├── Feed-Forward Network                # 前馈网络\n│   │   ├── Linear (768 -> 3072)            # 线性变换\n│   │   ├── GELU                            # GELU激活\n│   │   └── Linear (3072 -> 768)            # 线性变换\n│   └── Residual Connection                 # 残差连接\n├── Final Layer Norm                        # 最终层归一化\n└── Output (tied with wte.T)                # 输出（与wte.T共享权重）\n\nTotal Parameters / 总参数量:\n- 124M: 12 layers, 768 hidden, 12 heads\n- 355M: 24 layers, 1024 hidden, 16 heads\n- 774M: 36 layers, 1280 hidden, 20 heads\n- 1558M: 48 layers, 1600 hidden, 25 heads\n```",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 10. Key Takeaways / 关键要点\n\n1. **The entire GPT-2 forward pass is just ~40 lines of NumPy!**\n   \n   整个GPT-2前向传播只需约40行NumPy代码！\n\n2. **Core components / 核心组件:**\n   - `gelu`: Activation function / 激活函数\n   - `softmax`: Probability distribution / 概率分布\n   - `layer_norm`: Normalization / 归一化\n   - `linear`: Matrix multiplication / 矩阵乘法\n   - `attention`: The key innovation / 关键创新\n   - `mha`: Multi-head version / 多头版本\n   - `ffn`: Position-wise processing / 逐位置处理\n   - `transformer_block`: Combines attention + FFN / 组合注意力+FFN\n   - `gpt2`: Stack of blocks / 块的堆叠\n\n3. **One-line GPU/TPU acceleration with JAX!**\n   \n   使用JAX只需一行代码即可切换到GPU/TPU加速！\n\n4. **Limitations of this implementation / 此实现的局限性:**\n   - Inference only (no training) / 仅推理（无训练）\n   - No batching / 无批处理\n   - Greedy sampling only / 仅贪婪采样\n   - Slow (pure NumPy) / 速度慢（纯NumPy），但可通过JAX加速\n\n5. **The magic is in the pre-trained weights!**\n   \n   魔法在于预训练的权重！"
  },
  {
   "cell_type": "code",
   "source": "!pip install jax jaxlib",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "import jax\nimport jax.numpy as jnp\n\n# Check which device JAX is using\n# 检查JAX正在使用的设备\nprint(f\"JAX backend: {jax.default_backend()}\")\nprint(f\"JAX devices: {jax.devices()}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Redefine all model functions using jax.numpy instead of numpy\n# 用jax.numpy重新定义所有模型函数（代码完全一样，只是np现在指向jax.numpy）\n\n# --- The only change: np = jax.numpy ---\nnp = jnp\n\ndef gelu_jax(x):\n    return 0.5 * x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * x**3)))\n\ndef softmax_jax(x):\n    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n\ndef layer_norm_jax(x, g, b, eps=1e-5):\n    mean = np.mean(x, axis=-1, keepdims=True)\n    variance = np.var(x, axis=-1, keepdims=True)\n    x = (x - mean) / np.sqrt(variance + eps)\n    return g * x + b\n\ndef linear_jax(x, w, b):\n    return x @ w + b\n\ndef ffn_jax(x, c_fc, c_proj):\n    a = gelu_jax(linear_jax(x, **c_fc))\n    x = linear_jax(a, **c_proj)\n    return x\n\ndef attention_jax(q, k, v, mask):\n    return softmax_jax(q @ k.T / np.sqrt(q.shape[-1]) + mask) @ v\n\ndef mha_jax(x, c_attn, c_proj, n_head):\n    x = linear_jax(x, **c_attn)\n    qkv = np.split(x, 3, axis=-1)\n    qkv_heads = list(map(lambda x: np.split(x, n_head, axis=-1), qkv))\n    causal_mask = (1 - np.tri(x.shape[0], dtype=x.dtype)) * -1e10\n    out_heads = [attention_jax(q, k, v, causal_mask) for q, k, v in zip(*qkv_heads)]\n    x = np.hstack(out_heads)\n    x = linear_jax(x, **c_proj)\n    return x\n\ndef transformer_block_jax(x, mlp, attn, ln_1, ln_2, n_head):\n    x = x + mha_jax(layer_norm_jax(x, **ln_1), **attn, n_head=n_head)\n    x = x + ffn_jax(layer_norm_jax(x, **ln_2), **mlp)\n    return x\n\ndef gpt2_jax(inputs, wte, wpe, blocks, ln_f, n_head):\n    x = wte[inputs] + wpe[list(range(len(inputs)))]\n    for block in blocks:\n        x = transformer_block_jax(x, **block, n_head=n_head)\n    x = layer_norm_jax(x, **ln_f)\n    return x @ wte.T\n\ndef generate_jax(inputs, params, n_head, n_tokens_to_generate):\n    for _ in tqdm(range(n_tokens_to_generate), \"generating (JAX)\"):\n        logits = gpt2_jax(inputs, **params, n_head=n_head)\n        next_id = int(np.argmax(logits[-1]))\n        inputs.append(next_id)\n    return inputs[len(inputs) - n_tokens_to_generate :]\n\n# Restore np to numpy for other cells\n# 恢复np为numpy，避免影响其他cell\nimport numpy\nnp = numpy\n\nprint(\"JAX model functions defined!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Compare NumPy (CPU) vs JAX (GPU/TPU) speed\n# 对比NumPy（CPU）与JAX（GPU/TPU）的速度\nimport time\n\nprompt = \"Alan Turing theorized that computers would one day become\"\nn_tokens = 20\n\n# Load model (reuse if already loaded)\n# 加载模型（如果已经加载过则复用）\nencoder, hparams, params = load_encoder_hparams_and_params(\"124M\", \"models\")\ninput_ids = encoder.encode(prompt)\n\n# --- NumPy (CPU) ---\nprint(\"=\" * 50)\nprint(\"NumPy (CPU):\")\ninput_ids_np = list(input_ids)  # copy\nstart = time.time()\noutput_ids_np = generate(input_ids_np, params, hparams[\"n_head\"], n_tokens)\ntime_np = time.time() - start\nprint(f\"Output: {encoder.decode(output_ids_np)}\")\nprint(f\"Time: {time_np:.2f}s\")\n\n# --- JAX (GPU/TPU) ---\nprint(\"\\n\" + \"=\" * 50)\nprint(f\"JAX ({jax.default_backend().upper()}):\")\n\n# Convert params to JAX arrays for GPU acceleration\n# 将参数转换为JAX数组以启用GPU加速\ndef to_jax(d):\n    if isinstance(d, dict):\n        return {k: to_jax(v) for k, v in d.items()}\n    elif isinstance(d, list):\n        return [to_jax(v) for v in d]\n    else:\n        return jnp.array(d)\n\nparams_jax = to_jax(params)\n\ninput_ids_jax = list(input_ids)  # copy\nstart = time.time()\noutput_ids_jax = generate_jax(input_ids_jax, params_jax, hparams[\"n_head\"], n_tokens)\ntime_jax = time.time() - start\nprint(f\"Output: {encoder.decode(output_ids_jax)}\")\nprint(f\"Time: {time_jax:.2f}s\")\n\n# Summary\n# 总结\nprint(\"\\n\" + \"=\" * 50)\nprint(f\"Speedup / 加速比: {time_np / time_jax:.1f}x\")\nprint(f\"(Note: JAX's first run includes JIT compilation overhead)\")\nprint(f\"(注意：JAX首次运行包含JIT编译开销)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}